---
phase: 03-self-evolution-engine
plan: 02
type: execute
wave: 1
depends_on: ["03-01"]
files_modified:
  - shared/providers/github_models.py
  - shared/providers/llm_gateway.py
  - shared/providers/base_provider.py
  - orchestrator/orchestrator_service.py
  - tests/unit/providers/test_github_models.py
  - tests/unit/providers/test_llm_gateway.py
  - tests/unit/providers/test_fallback_chain.py
autonomous: true
user_setup:
  - "Provide GITHUB_MODELS_TOKEN via env for local smoke (can be fake for unit tests)"

must_haves:
  truths:
    - "Backend codegen uses GitHub Models inference endpoint, not Copilot IDE agent mode"
    - "Gateway enforces response_format json_schema and rejects non-conforming outputs"
    - "Routing supports purpose lanes: codegen, repair, critic (config-driven)"
    - "Provider/model outage triggers deterministic fallback or 'paused' job state"
    - "Gateway is a module within shared/providers/ — not a separate microservice"
    - "Retry/backoff applies only to 429/5xx/timeouts — not auth or schema errors"
  artifacts:
    - path: "shared/providers/github_models.py"
      provides: "GitHub Models chat/completions client with org attribution"
      contains: "GitHubModelsProvider"
    - path: "shared/providers/llm_gateway.py"
      provides: "Routing layer with purpose lanes and schema enforcement"
      contains: "LLMGateway, RoutingPolicy"
    - path: "tests/unit/providers/test_fallback_chain.py"
      provides: "Deterministic fallback behavior tests"
      contains: "test_fallback_on_model_error"
  key_links:
    - from: "shared/providers/llm_gateway.py"
      to: "shared/providers/github_models.py"
      via: "gateway routes codegen/repair/critic to configured providers"
      pattern: "route_request"
    - from: "shared/providers/llm_gateway.py"
      to: "shared/modules/contracts.py"
      via: "enforces GeneratorResponseContract on LLM outputs"
      pattern: "validate_response"
    - from: "orchestrator/orchestrator_service.py"
      to: "shared/providers/llm_gateway.py"
      via: "builder calls gateway for structured code generation"
      pattern: "gateway.generate"
---

<objective>
Ship the LLM provider wrapper that normalizes auth, org attribution, schema enforcement, routing, budgets, and retries.

Purpose: Provide a stable code-generation surface for the builder by wrapping GitHub Models API with purpose-based routing and strict output validation.
Output: Gateway module ready to plug into the builder orchestrator in Wave 2.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/03-self-evolution-engine/03-RESEARCH.md
@shared/providers/base_provider.py
@shared/providers/openai_provider.py
@orchestrator/orchestrator_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: GitHub Models provider client</name>
  <files>shared/providers/github_models.py, shared/providers/base_provider.py, tests/unit/providers/test_github_models.py</files>
  <action>
Implement GitHub Models provider following existing provider pattern (OpenAI, Anthropic):
- Extend BaseProvider with GitHubModelsProvider
- Call https://models.github.ai/inference/chat/completions endpoint
- Support org-attributed path variant for enterprise billing
- Include required headers: Authorization Bearer, X-GitHub-Api-Version, Accept
- Support response_format with json_schema for structured outputs
- Implement retry/backoff for 429/5xx/timeouts with bounded attempts (max 3)
- No retry on 401/403 (auth errors) or 400 (schema errors)

Add unit tests with mocked HTTP responses for success, 429 retry, timeout, and auth failure.
  </action>
  <verify>
Run: `cd /Users/sertanavdan/Documents/Software/AI/gRPC_llm && python -m pytest tests/unit/providers/test_github_models.py -v --tb=short`
  </verify>
  <done>
GitHub Models client works with both attributed and unattributed endpoints, retries on transient failures, and returns structured JSON.
  </done>
</task>

<task type="auto">
  <name>Task 2: LLM Gateway routing layer</name>
  <files>shared/providers/llm_gateway.py, tests/unit/providers/test_llm_gateway.py</files>
  <action>
Implement gateway module:
- RoutingPolicy: purpose lanes (codegen, repair, critic) with config-driven model selection
- Route table: each lane has ordered list of model preferences
- generate() method: accepts purpose + messages + schema, routes to correct provider
- Response validation: parse output against GeneratorResponseContract from shared/modules/contracts.py
- Reject non-conforming outputs with structured error (not silent fallback)
- Budget controls: max tokens per request, per-job token budget tracking
- Seed support for reproducibility

Add unit tests for routing selection, schema validation, and budget enforcement.
  </action>
  <verify>
Run: `cd /Users/sertanavdan/Documents/Software/AI/gRPC_llm && python -m pytest tests/unit/providers/test_llm_gateway.py -v --tb=short`
  </verify>
  <done>
Gateway routes requests by purpose, enforces response schema, and respects token budgets.
  </done>
</task>

<task type="auto">
  <name>Task 3: Fallback chain + orchestrator wiring</name>
  <files>shared/providers/llm_gateway.py, orchestrator/orchestrator_service.py, tests/unit/providers/test_fallback_chain.py</files>
  <action>
Implement fallback behavior:
- When primary model fails: select next model in lane's preference list
- When all models in lane fail: return structured error with "paused" job state recommendation
- Fallback is deterministic: same failure condition always selects same next model
- Log all fallback transitions for debugging

Wire gateway into orchestrator:
- Register LLMGateway as available provider for build operations
- Ensure existing provider routing (local, OpenAI, etc.) is not disrupted
- Build tool can request generation via gateway.generate(purpose="codegen", ...)

Add integration-style unit tests: model fails → next selected; all fail → structured error returned.
  </action>
  <verify>
Run: `cd /Users/sertanavdan/Documents/Software/AI/gRPC_llm && python -m pytest tests/unit/providers/test_fallback_chain.py -v --tb=short`
  </verify>
  <done>
Routing policy selects models by purpose and falls back deterministically. Orchestrator can invoke gateway for structured code generation.
  </done>
</task>

</tasks>

<verification>
- GitHub Models client handles success, retry, timeout, and auth rejection
- Gateway routes codegen/repair/critic to configured models
- Schema enforcement rejects non-conforming LLM outputs
- Fallback chain selects next model on failure
- Existing provider routing (local, OpenAI, Anthropic) is unchanged
</verification>

<success_criteria>
- Gateway is stable enough to plug into the builder orchestrator in Wave 2
- REQ-013: programmatic module generation engine exists (backend-compatible inference surface)
- REQ-016: deterministic schema enforcement at the gateway prevents garbage outputs from entering validation
</success_criteria>

<output>
After completion, create `.planning/phases/03-self-evolution-engine/03-02-SUMMARY.md`
</output>
