---
phase: 03-self-evolution-engine
plan: 05
type: execute
wave: 3
depends_on: ["03-03", "03-04"]
files_modified:
  - tools/builtin/feature_test_harness.py
  - tools/builtin/chart_validator.py
  - shared/modules/templates/test_template.py
  - shared/modules/scenarios/registry.py
  - shared/modules/scenarios/rest_api.py
  - shared/modules/scenarios/oauth2_flow.py
  - shared/modules/scenarios/paginated_api.py
  - shared/modules/scenarios/file_parser.py
  - shared/modules/scenarios/rate_limited_api.py
  - tests/contract/test_registration_contract.py
  - tests/contract/test_output_schema_contract.py
  - tests/feature/test_auth_api_key.py
  - tests/feature/test_oauth_refresh.py
  - tests/feature/test_rate_limit_429.py
  - tests/feature/test_pagination_cursor.py
  - tests/feature/test_schema_drift_detection.py
  - tests/feature/test_chart_artifacts.py
  - tests/scenarios/test_scenario_regression.py
  - Makefile
autonomous: true
user_setup:
  - "Optional: set NEXUS_LIVE_TESTS=1 + allowed_hosts for live smoke tests (still sandboxed)"

must_haves:
  truths:
    - "VALIDATED requires all critical contract tests + declared feature suites pass"
    - "Feature suites are capability-driven (manifest declares auth/pagination/rate-limit/chart outputs)"
    - "Chart outputs are validated via artifact envelope + structural integrity tiers"
    - "Scenario library covers 5+ curated build patterns exercised in CI"
    - "Generated tests include generic contract tests and feature-specific tests"
    - "Tests are deterministic: no real sleeps, stable fixtures, mock transport by default"
  artifacts:
    - path: "tools/builtin/feature_test_harness.py"
      provides: "Capability-driven test suite selector and runner"
      contains: "select_suites, run_feature_tests"
    - path: "tools/builtin/chart_validator.py"
      provides: "Chart artifact envelope validation with structural + semantic tiers"
      contains: "validate_chart, ChartValidationResult"
    - path: "shared/modules/scenarios/registry.py"
      provides: "Scenario library registry with 5+ curated build patterns"
      contains: "ScenarioRegistry, get_scenario"
    - path: "shared/modules/templates/test_template.py"
      provides: "Generic + feature-specific generated test code templates"
      contains: "generate_test_code"
    - path: "Makefile"
      provides: "Phase 3 regression command"
      contains: "test-self-evolution"
  key_links:
    - from: "tools/builtin/feature_test_harness.py"
      to: "shared/modules/manifest_schema.json"
      via: "reads manifest capabilities to select which test suites to run"
      pattern: "select_suites"
    - from: "tools/builtin/module_validator.py"
      to: "tools/builtin/feature_test_harness.py"
      via: "validator delegates feature-specific testing to harness"
      pattern: "run_feature_tests"
    - from: "tools/builtin/chart_validator.py"
      to: "shared/modules/output_contract.py"
      via: "chart validation references artifact envelope from output contract"
      pattern: "validate_chart"
---

<objective>
Raise the quality gate from "it runs" to "it behaves" — with feature-specific testing, visualization validation, and a curated scenario library for builder regression.

Purpose: Ensure generated adapters handle real-world patterns (auth, pagination, rate-limits, charts) and the builder itself is regression-tested against curated scenarios.
Output: Validator selects and runs suites based on manifest declarations. Scenario library exercises 5+ build patterns in CI.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/03-self-evolution-engine/03-RESEARCH.md
@shared/modules/templates/test_template.py
@tools/builtin/module_builder.py
@tools/builtin/module_validator.py
@shared/modules/output_contract.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Contract test suites</name>
  <files>tests/contract/test_registration_contract.py, tests/contract/test_output_schema_contract.py, shared/modules/templates/test_template.py</files>
  <action>
Implement generic contract tests (Class A from research):

Registration contract:
- Module registers correctly via decorator
- Module appears in registry after registration
- Duplicate registration is handled

Output schema contract:
- Adapter produces AdapterRunResult envelope
- Required fields present (contract, run, status, data_points)
- Error codes use standardized values

Update test_template.py to generate these contract tests automatically for any new module.
Ensure generated tests use mock transport (no real HTTP).

Add tests that verify the generator emits valid contract test files.
  </action>
  <verify>
Run: `cd /Users/sertanavdan/Documents/Software/AI/gRPC_llm && python -m pytest tests/contract/ -v --tb=short`
  </verify>
  <done>
Contract tests validate registration, output schema, and error handling for any generated module.
  </done>
</task>

<task type="auto">
  <name>Task 2: Feature-specific test harness</name>
  <files>tools/builtin/feature_test_harness.py, tests/feature/test_auth_api_key.py, tests/feature/test_oauth_refresh.py, tests/feature/test_rate_limit_429.py, tests/feature/test_pagination_cursor.py, tests/feature/test_schema_drift_detection.py</files>
  <action>
Implement capability-driven test suite selection:

Feature test harness:
- Read manifest capabilities to determine which suites apply
- auth type "api_key" -> run auth_api_key suite
- auth type "oauth2" -> run oauth_refresh suite
- capability "pagination" -> run pagination_cursor suite
- capability "rate_limited" -> run rate_limit_429 suite
- Always run schema_drift_detection

Feature test implementations:
- auth_api_key: verify 401 on bad key, 200 on valid key, key in correct header
- oauth_refresh: verify token exchange, refresh flow, expired token handling
- rate_limit_429: verify backoff on 429, retry succeeds, max retries respected
- pagination_cursor: verify page iteration, max_pages guard, repeated cursor detection
- schema_drift: classify additive vs breaking changes, alert on breaking

All tests use mock HTTP transport. No real network calls.
Deterministic patterns: no real sleeps, stable fixtures.
  </action>
  <verify>
Run: `cd /Users/sertanavdan/Documents/Software/AI/gRPC_llm && python -m pytest tests/feature/ -v --tb=short`
  </verify>
  <done>
Feature harness selects suites based on manifest capabilities. All feature tests pass with mocked transport.
  </done>
</task>

<task type="auto">
  <name>Task 3: Visualization artifact validation</name>
  <files>tools/builtin/chart_validator.py, tests/feature/test_chart_artifacts.py</files>
  <action>
Implement chart output validation:

Chart artifact envelope:
- file metadata: mime type, byte size, sha256, dimensions (if image)
- data summary: series names, data point count, value ranges
- semantic summary: title, axis labels, legend entries
- provenance: rendering engine, version, backend

Validation tiers:
- Tier 1 (structural): file exists, decodes successfully, metadata is sane
- Tier 2 (semantic): expected series present, data binding matches, labels non-empty
- Tier 3 (optional): deterministic rendering hash (pinned backend/fonts, opt-in only)

Fix hints for chart failures:
- Missing series -> "Expected series 'X' not found in chart output"
- Decode failure -> "Chart file corrupt or wrong mime type"
- Empty data -> "Chart produced with no data points"

Add feature tests for chart validation at each tier.
  </action>
  <verify>
Run: `cd /Users/sertanavdan/Documents/Software/AI/gRPC_llm && python -m pytest tests/feature/test_chart_artifacts.py -v --tb=short`
  </verify>
  <done>
Chart artifact schema + validation tiers produce actionable fix hints. Visualization generation is first-class in generated modules.
  </done>
</task>

<task type="auto">
  <name>Task 4: Scenario library + CI regression</name>
  <files>shared/modules/scenarios/registry.py, shared/modules/scenarios/rest_api.py, shared/modules/scenarios/oauth2_flow.py, shared/modules/scenarios/paginated_api.py, shared/modules/scenarios/file_parser.py, shared/modules/scenarios/rate_limited_api.py, tests/scenarios/test_scenario_regression.py, Makefile</files>
  <action>
Implement curated scenario library per ROADMAP deliverable #4:

Scenario registry:
- ScenarioRegistry: loads all registered scenarios
- Each scenario defines: NL intent, expected adapter shape, manifest capabilities, test suite requirements, known edge cases

5+ curated scenarios:
1. rest_api: Simple REST API (weather/stocks style) — GET endpoint, API key auth, JSON response
2. oauth2_flow: OAuth2 API (calendar/CRM style) — token exchange, refresh, scoped access
3. paginated_api: Paginated results (search/list style) — cursor pagination, max pages guard
4. file_parser: File-based data (CSV/Excel style) — local file parsing, column mapping
5. rate_limited_api: Rate-limited API (gaming/social style) — 429 handling, backoff, retry

Regression test:
- For each scenario: verify builder can produce a scaffold that passes contract tests
- Does NOT require live LLM — uses pre-recorded expected outputs where possible
- Add `make test-self-evolution` target that runs all Phase 3 tests including scenarios

Add scenario count assertion: CI fails if < 5 scenarios registered.
  </action>
  <verify>
Run: `cd /Users/sertanavdan/Documents/Software/AI/gRPC_llm && python -m pytest tests/scenarios/ -v --tb=short`
  </verify>
  <done>
5+ scenario templates exercised in CI. Scenario library provides builder regression coverage and documents expected adapter shapes.
  </done>
</task>

</tasks>

<verification>
- Contract tests validate registration + output schema for generated modules
- Feature harness selects suites based on manifest capabilities
- Chart validation produces actionable fix hints at each tier
- Scenario library has >= 5 scenarios, all passing in CI
- `make test-self-evolution` runs all Phase 3 tests and reports pass/fail
- All tests are deterministic: no real network, no real sleeps
</verification>

<success_criteria>
- REQ-013: supports REST/OAuth/pagination/rate-limits + visualization artifacts + scenario library
- REQ-016: generated tests include generic + feature-specific suites, deterministic under sandbox policy
- ROADMAP: "5+ scenario templates exercised in CI" is satisfied
- Quality gate raised from "it runs" to "it handles auth, pagination, rate-limits, and charts correctly"
</success_criteria>

<output>
After completion, create `.planning/phases/03-self-evolution-engine/03-05-SUMMARY.md`
</output>
