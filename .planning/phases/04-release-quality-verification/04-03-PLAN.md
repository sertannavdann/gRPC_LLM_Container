---
phase: 04-release-quality-verification
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - scripts/verify.sh
  - shared/billing/latency_snapshot.py
  - tests/unit/test_latency_snapshot.py
  - Makefile
autonomous: true

must_haves:
  truths:
    - "make verify runs all test tiers in sequence and exits non-zero on first failure"
    - "Latency snapshot captures p50/p95/p99 for key endpoints and persists as JSON"
    - "Structured pass/fail report printed to stdout at end of verification"
  artifacts:
    - path: "scripts/verify.sh"
      provides: "Unified verification orchestration script"
      min_lines: 60
    - path: "shared/billing/latency_snapshot.py"
      provides: "Latency percentile calculator and JSON snapshot writer"
      exports: ["LatencySnapshot", "record_latencies", "write_snapshot"]
    - path: "tests/unit/test_latency_snapshot.py"
      provides: "Unit tests for latency snapshot computation"
      min_lines: 40
  key_links:
    - from: "Makefile"
      to: "scripts/verify.sh"
      via: "make verify target calls bash scripts/verify.sh"
      pattern: "verify.*scripts/verify"
    - from: "scripts/verify.sh"
      to: "shared/billing/latency_snapshot.py"
      via: "invokes python -m shared.billing.latency_snapshot after tests pass"
      pattern: "latency_snapshot"
---

<objective>
Create the unified `make verify` command that runs all test tiers and records a latency snapshot (REQ-028).

Purpose: Single command for release confidence. Runs unit → contract → integration → admin API → feature → showroom → latency snapshot. Exits 0 on healthy system, non-zero on failure. Latency snapshot persisted as JSON artifact.

Output: `scripts/verify.sh`, `shared/billing/latency_snapshot.py`, `make verify` target, and latency snapshot unit tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-release-quality-verification/04-01-SUMMARY.md
@.planning/phases/04-release-quality-verification/04-02-SUMMARY.md
@Makefile
@scripts/showroom_test.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Latency Snapshot Module</name>
  <files>shared/billing/latency_snapshot.py, tests/unit/test_latency_snapshot.py</files>
  <action>
    **shared/billing/latency_snapshot.py**:
    - `LatencySnapshot` dataclass: timestamp, orchestrator_version (from `git rev-parse --short HEAD`), endpoints dict, total_tests, passed, failed, duration_s
    - `compute_percentiles(latencies: list[float]) -> dict` — returns {"p50_ms": float, "p95_ms": float, "p99_ms": float} using sorted-index method (no numpy dependency)
    - `record_latencies(endpoints: dict[str, list[float]]) -> dict[str, dict]` — takes {endpoint_name: [latency_ms, ...]} and returns {endpoint_name: {p50, p95, p99}}
    - `write_snapshot(snapshot: LatencySnapshot, path: str = "data/verify_snapshot.json") -> None` — writes JSON to disk, creates parent dir if needed
    - `probe_endpoints(base_url: str, endpoints: list[str], samples: int = 5) -> dict[str, list[float]]` — HTTP GET each endpoint N times, collect latencies. Uses urllib.request (stdlib, no requests dependency). Adds X-API-Key header if API_KEY env var set.
    - CLI entry: `if __name__ == "__main__"` block that probes default endpoints and writes snapshot

    **tests/unit/test_latency_snapshot.py**:
    - Test `compute_percentiles` with known data: [10, 20, 30, 40, 50] → p50=30
    - Test `compute_percentiles` with single value: [100] → all percentiles = 100
    - Test `compute_percentiles` with empty list: returns zeros
    - Test `record_latencies` maps multiple endpoints
    - Test `write_snapshot` creates JSON file at tmp_path
    - Test snapshot JSON is valid and contains expected keys
  </action>
  <verify>
    `python -m pytest tests/unit/test_latency_snapshot.py -v --tb=short` — all pass.
    `python -c "from shared.billing.latency_snapshot import LatencySnapshot, compute_percentiles, write_snapshot; print('ok')"` succeeds.
  </verify>
  <done>
    Latency snapshot module computes p50/p95/p99 percentiles without numpy.
    JSON snapshot writer creates structured artifact.
    6+ unit tests covering percentile math and serialization.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unified Verify Script + Makefile Target</name>
  <files>scripts/verify.sh, Makefile</files>
  <action>
    **scripts/verify.sh** — orchestration script:
    ```
    #!/usr/bin/env bash
    set -euo pipefail
    # NEXUS Release Verification Pipeline
    #
    # Runs all test tiers in sequence, stops on first failure.
    # Produces structured pass/fail report and latency snapshot.
    ```
    Steps (each prints colored header before running):
    1. Unit tests: `python -m pytest tests/unit/ -q --tb=line`
    2. Contract tests: `python -m pytest tests/contract/ -q --tb=line`
    3. Integration tests: `python -m pytest tests/integration/ -q --tb=line`
    4. Feature tests: `python -m pytest tests/feature/ -q --tb=line`
    5. Scenario tests: `python -m pytest tests/scenarios/ -q --tb=line` (skip if dir missing)
    6. Showroom: `bash scripts/showroom_test.sh` (skip if services not running — check /admin/health first)
    7. Latency snapshot: `python -m shared.billing.latency_snapshot` (skip if services not running)

    Each step:
    - Prints step name with color
    - Runs command, captures exit code
    - On failure: prints FAIL, records failed step, continues to next if `--no-bail` flag, otherwise exits
    - On success: prints PASS, increments pass counter

    At end:
    - Print structured summary table: step name, status (PASS/FAIL), duration
    - Print total: X/Y steps passed
    - Exit 0 if all passed, 1 if any failed
    - If latency snapshot was written, print path to JSON artifact

    Support flags:
    - `--no-bail` — run all steps even if one fails
    - `--skip-showroom` — skip showroom + latency (for CI without Docker)
    - Default: bail on first failure

    **Makefile** — add `verify` target (near the test targets section, around line 800):
    ```makefile
    verify:
    	@printf '$(BOLD)$(CYAN)Running NEXUS Release Verification...$(RESET)\n'
    	@bash scripts/verify.sh
    ```
    Also add `verify` to the `.PHONY` list (line ~54) and to the help section.

    Make script executable: `chmod +x scripts/verify.sh`
  </action>
  <verify>
    `make verify --dry-run` shows the correct command.
    `bash scripts/verify.sh --skip-showroom` runs test tiers and prints summary.
  </verify>
  <done>
    `make verify` runs unit → contract → integration → feature → scenario → showroom → latency snapshot.
    Exits 0 on all-pass, non-zero on failure.
    Structured summary printed.
    Latency snapshot written to data/verify_snapshot.json when services are running.
  </done>
</task>

</tasks>

<verification>
- `make verify --dry-run` shows `bash scripts/verify.sh`
- `bash scripts/verify.sh --skip-showroom` completes with structured summary
- `python -m pytest tests/unit/test_latency_snapshot.py -v` passes
- `data/verify_snapshot.json` created when probing live services
</verification>

<success_criteria>
- `make verify` is a single-command release confidence gate
- All test tiers run in order; first failure stops execution (default) or continues (--no-bail)
- Latency snapshot records p50/p95/p99 as JSON artifact
- Structured pass/fail report shows step-by-step results
- Works in CI (--skip-showroom for no-Docker environments)
</success_criteria>

<output>
After completion, create `.planning/phases/04-release-quality-verification/04-03-SUMMARY.md`
</output>
