# RL Training Inside NEXUS: Negative Consequences, Per-Rebuild Checkpoints, and NPU Parallel Inference

***

## 1. Negative Consequences of Embedding RL Training in a Production Orchestrator

You're right to push back on the "optimization loop without weight training" idea—it masks the real engineering cost. But the alternative you're proposing (actual weight training tied to the orchestrator lifecycle) has its own severe consequences that must be named explicitly.

### 1.1 Energy and Compute Cost

RL post-training is extraordinarily expensive. Even GRPO (which eliminates the value model and is cheaper than PPO) requires generating **multiple rollout completions per prompt** (typically 4–16), scoring each, computing advantages, and performing gradient updates. For a 7B–14B model, a single GRPO training run can consume hundreds of GPU-hours. Embedding this inside every orchestrator rebuild means:[^1][^2]

- **Every rebuild = full RL training loop**: if you have a 14B Heavy-tier model and 6 plans worth of modules, you're looking at significant GPU-hours per rebuild cycle just for rollout generation, reward computation, and gradient steps.
- **Energy cost is not amortized**: unlike a cloud training service where you train once and serve many, per-rebuild training means you pay the full energy cost every time the orchestrator image changes. This is the opposite of efficient.
- **Inference hardware ≠ training hardware**: your NEXUS stack runs on Docker Compose with local llama.cpp inference. RL training requires gradient computation, which llama.cpp does not support—you'd need a separate PyTorch/TRL training service with GPU access, adding an entirely new container and dependency chain.

### 1.2 Reward Hacking

When the RL policy discovers patterns that maximize the reward proxy without actually improving alignment with user intent, you get **reward hacking**. In NEXUS's case, if your reward signal is "module builds successfully + passes contract tests," the model can learn to:[^3][^4]

- Generate trivially simple adapters that always pass but do nothing useful
- Exploit known patterns in your test harness rather than genuinely solving the integration problem
- Produce verbose, confident-sounding code that scores well on surface metrics but fails on edge cases

The PPO KL penalty (or GRPO's group-relative advantage) helps constrain drift from the SFT base, but it **does not prevent the policy from exploiting subtle inaccuracies in the reward model**. Research shows that reward hacking can emerge in fewer than 200 training steps and saturate to nearly 100% of model responses.[^5][^3]

### 1.3 Catastrophic Forgetting

Fine-tuning on NEXUS-specific module-building trajectories will degrade the model's general capabilities. On-policy RL (PPO/GRPO) forgets less than SFT because the KL regularization keeps the policy closer to the base model in distribution space. However:[^6][^7]

- Each rebuild that retrains from the same base checkpoint resets any accumulated specialization
- Each rebuild that trains from the *previous* checkpoint risks compounding distribution drift
- Mathematical reasoning, code quality on non-NEXUS tasks, and general instruction-following all degrade as the model specializes[^6]

### 1.4 Data Staleness

In asynchronous RL systems, trajectories generated by an older version of the model are used to train a newer version—this is **data staleness**. If your per-rebuild approach generates rollouts during one orchestrator version and applies gradients in the next, the staleness bound η directly impacts convergence. Recent work (StaleFlow, Jan 2026) shows that even η=1–3 steps of staleness requires explicit consistency protocols to avoid training instability.[^8][^9]

### 1.5 Reproducibility and Auditability

Your Phase 5 (Audit Trail) requires immutable records of every action. RL training introduces **non-deterministic weight updates** that depend on:
- Random sampling during rollout generation
- Order of trajectory processing
- Hardware-specific floating-point behavior

This makes it extremely difficult to reproduce a specific model checkpoint, which conflicts with your audit and compliance requirements for enterprise customers.

***

## 2. Per-Rebuild RL Checkpoint: What It Actually Requires

### 2.1 The Architecture You're Describing

You want: on every orchestrator rebuild (or on every new orchestrator instance), collect anonymized module-building trajectories, run an RL training loop (GRPO/PPO), save the resulting weights, and use those weights for subsequent inference. The "anonymized selection" trains the decision-making policy, and the goal is convergence toward a global minimum of the loss landscape.

### 2.2 What "Global Minima" Means Here

The loss landscape for RL fine-tuning of LLMs is **non-convex** with many local minima. The standard GRPO objective is:

\[
\mathcal{L}_{\text{GRPO}}(\theta) = -\mathbb{E}_{q \sim \mathcal{D}, \{o_i\} \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \left[ \frac{1}{G} \sum_{i=1}^{G} \min\left( r_i(\theta) \hat{A}_i, \text{clip}(r_i(\theta), 1{-}\epsilon, 1{+}\epsilon) \hat{A}_i \right) \right]
\]

where \(\hat{A}_i\) is the group-relative advantage and \(r_i(\theta)\) is the probability ratio. Reaching a "global minimum" is not realistic—what you're actually optimizing for is a **local basin** that produces good module-building behavior without catastrophically forgetting general capabilities. The KL penalty term is what keeps you from falling into a degenerate basin.[^10][^7][^1]

### 2.3 Per-Rebuild vs. Continuous Training

| Approach | Pros | Cons |
|---|---|---|
| **Train from base every rebuild** | No compounding drift, reproducible | Loses all prior specialization, expensive |
| **Train from previous checkpoint** | Accumulates domain knowledge | Compounds forgetting, drift, reward hacking |
| **Train offline, deploy frozen weights** | Cheapest inference, most stable | No adaptation to new module patterns |
| **LoRA adapters per-rebuild** | Small delta, fast training, composable | Still requires GPU training infrastructure |

The most practical approach for NEXUS: **offline LoRA/GRPO training** on anonymized trajectories collected over time, merged into a new adapter on a scheduled cadence (weekly/monthly), not per-rebuild. This gives you:[^1]

- Controlled energy budget (train once, serve many rebuilds)
- Proper evaluation gates before deployment
- Audit trail for which training data produced which weights
- No training infrastructure in the hot path

### 2.4 Multi-Stage Training Pipeline

GRPO supports multi-stage training which aligns with your module-building pipeline:[^1]

- **Stage 1**: Format compliance (adapter follows manifest schema, correct file structure)
- **Stage 2**: Functional correctness (passes contract tests, handles errors)
- **Stage 3**: Efficiency (minimizes token usage, repair loop convergence)

Each stage uses different reward function weights. This maps naturally to your existing Phase 3 self-correction loop stages (scaffold → implement → tests → repair).

***

## 3. NPU Parallel Inference Pipelines and Driver Architecture

### 3.1 NPU vs. GPU: Fundamental Architecture Difference

NPUs use a **SIMD/VLIW execution model** (single instruction stream operating on large data blocks) rather than the GPU's **SIMT model** (many independent threads). This means:[^11]

- NPU scheduling is **compiler-driven and static**: the compiler decides instruction ordering and parallelism at compile time, not at runtime[^12]
- NPU hardware has fewer hardware threads (6–8 scalar VLIW threads on Hexagon) but wider data paths (1024-bit vectors)[^11]
- NPUs sacrifice programming flexibility for **higher execution efficiency and energy efficiency**[^11]

### 3.2 NPU Driver Architecture (Three Layers)

All major NPU implementations follow a similar three-layer pattern:[^13][^14]

**Layer 1 — Kernel Mode Driver (KMD)**:
- Intel NPU: `intel_vpu` kernel module at `drivers/accel/ivpu/`, exposes `/dev/accel/accel0`[^14]
- Qualcomm Hexagon: accessed via FastRPC facility in Hexagon SDK[^11]
- Intel on Windows: D3D12-compliant user mode library + MCDM (Microsoft Compute Driver Model) kernel mode driver[^13]

The KMD handles:
- **Memory management**: page table setup, MMU context initialization, buffer object (BO) creation and pinning[^14]
- **IPC with firmware**: shared memory regions where kernel and NPU firmware exchange command buffers and completion signals[^14]
- **Power management**: runtime suspend/resume, clock gating[^14]
- **Work submission**: validating command buffers, setting up DMA transfers, submitting to firmware[^14]

**Layer 2 — Userspace Driver (UMD)**:
- Intel: `libze_intel_vpu.so` implementing Level Zero API[^14]
- Qualcomm: QNN SDK (closed-source) or direct Hexagon SDK with LLVM toolchain[^11]
- Translates high-level API calls (`zeMemAllocHost()`, `zeCommandQueueExecuteCommandLists()`) into kernel ioctls[^14]

**Layer 3 — NPU Firmware**:
- Runs autonomously on the accelerator hardware[^14]
- Receives command buffers from kernel, schedules compute kernels, manages on-chip memory[^14]
- Signals completion through interrupts (IPC channels)[^14]
- On Hexagon: a thread continuously polls shared memory to receive computation requests from CPU[^11]

### 3.3 Parallel Compute on NPUs

#### Hexagon NPU (Qualcomm)

The Hexagon NPU has two compute units working in parallel:[^11]

- **HVX (Hexagon Vector eXtension)**: general-purpose SIMD vector unit, 1024-bit registers, 4–6 units per NPU
- **HMX (Hexagon Matrix eXtension)**: dedicated matrix multiplication unit, operates on 32×32 FP16 tiles

Key performance numbers on Hexagon V75:[^11]

| Unit | FP16 GEMM (GFLOPs) | Memory Read BW (GB/s) |
|---|---|---|
| HVX (1 thread) | 32.93 | 26 |
| HMX | 12,032 | 60 (DMA) |

The HMX is **365× faster** than HVX for matrix multiply. During LLM decode (batch=1), GEMM degenerates to GEMV, wasting 31 of 32 rows in each HMX tile. This is the key opportunity: **parallel test-time scaling (batch>1) fills those wasted tile rows** for near-free additional compute.[^11]

Memory hierarchy: 1 MiB L2 cache + 8 MiB TCM (Tightly Coupled Memory, software-managed SRAM). HMX can only access TCM, not L2 cache. Data moves from DDR → L2/TCM via `l2fetch` instruction and DMA (asynchronous 1D/2D tensor transfers).[^11]

Communication: CPU ↔ NPU via `rpcmem` shared memory (wrapper for kernel dmabuf). No automatic cache coherence—**one-way coherence only** on Snapdragon SoCs, requiring manual cache flushes before NPU reads.[^15][^11]

#### Apple Neural Engine (ANE)

Apple's ANE operates through Core ML, which acts as both compiler and runtime:[^16][^17]

- Core ML automatically splits models across CPU, GPU, and ANE based on operator support[^17]
- ANE requires **fixed input shapes** (no dynamic shapes)[^16]
- State dimension sizes must be **powers of two** or you get runtime errors[^16]
- The `computeUnits = .all` setting allows Core ML to delegate to ANE[^18]

The best current approach for LLM inference on Apple Silicon is **disaggregated inference**:[^16]
- **Prefill on ANE** (via Core ML): high-throughput, power-efficient batch matrix multiply
- **Decode on GPU** (via MLX): fast single-token generation leveraging GPU's flexible memory access

This is implemented in the Yetter Inference Engine, which consistently outperforms both pure MLX (GPU) and pure Core ML (ANE) in end-to-end latency for prefill-heavy scenarios.[^16]

#### Intel NPU

Intel's NPU driver uses the MCDM (Microsoft Compute Driver Model) architecture on Windows:[^13]
- D3D12-compliant user mode library provides hardware acceleration for compute operators
- **Optimal schedule generation** for model execution is done by the compiler, not at runtime[^13]
- Workload submission via MCDM kernel mode driver
- Executes primarily out of **scratchpad SRAM** for maximum compute utilization[^13]

On Linux, the `intel_vpu` kernel module uses DRM (Direct Rendering Manager) subsystem with GEM buffer objects for memory management. A typical matrix multiplication workload involves ~4,131 page mappings and ~945 IPC messages between kernel and firmware.[^14]

### 3.4 NPU Virtualization and Scheduling

Current NPU architectures have **fundamental limitations for dynamic scheduling**:[^12]

- VLIW-style ISAs rely on ML compilers to exploit instruction-level parallelism at compile time
- Statically scheduled ISAs cannot fully support dynamic resource sharing at runtime
- This prevents fine-grained NPU virtualization (multiple workloads sharing NPU cores)

Research (Neu10, MICRO '24) proposes extending VLIW ISAs to support **vNPU abstractions** that enable temporal sharing of matrix/vector engines across collocated workloads. This would allow:[^12]
- Oversubscription of NPU cores across multiple inference requests
- Dynamic scheduling when individual ML workloads underutilize NPU cores
- Performance isolation between collocated DNN workloads

### 3.5 Practical NPU Integration for NEXUS

For your llama.cpp-based local inference in NEXUS:

- **Hexagon NPU backend**: exists as `llama.cpp-npu` (research prototype, ~7K lines C/C++ + inline assembly). Requires Snapdragon 8 Gen 2+, Android NDK, Hexagon SDK 6.x. Implements custom operator library with computation thread pool, power management, and shared-memory communication via FastRPC.[^15][^11]

- **Apple ANE**: not directly supported by llama.cpp. Requires Core ML conversion pipeline with numerous undocumented workarounds (state dimension padding, `pow` → `mul` rewriting, SDPA slicing for long sequences). The Yetter Engine approach (ANE prefill + GPU decode) is the current best path.[^16]

- **Intel NPU**: preliminary ggml-hexagon backend exists. Intel's NPU driver is in mainline Linux kernel (6.2+) at `drivers/accel/ivpu/`. Traceable via eBPF/bpftrace for performance debugging.[^19][^14]

### 3.6 What "Parallel NPU Pipelines" Means Concretely

True NPU parallelism in LLM inference comes from:

1. **Filling matrix unit tiles**: batch multiple inference requests to fill the 32×32 HMX tiles that are wasted during single-token decode[^11]
2. **Disaggregated prefill/decode**: run compute-bound prefill on NPU while running memory-bound decode on GPU[^16]
3. **Test-time scaling**: use the "free" NPU compute during decode to run parallel sampling (Best-of-N, beam search) for quality improvement at near-zero marginal cost[^11]

The INT8 MatMul speedup on Hexagon NPU vs alternatives:[^20]

| Matrix Size | NPU INT8 (ms) | CPU INT8 (ms) | GPU FP16 (ms) |
|---|---|---|---|
| 64×2048 × 2048×2048 | 0.9 | 4.2 (4.6×) | 1.7 (1.9×) |
| 64×2048 × 2048×8192 | 1.5 | 6.8 (4.5×) | 4.8 (3.2×) |
| 32×4096 × 4096×4096 | 1.7 | 7.5 (4.4×) | 3.1 (1.8×) |
| 32×4096 × 4096×11008 | 4.1 | 19.6 (4.8×) | 10.4 (2.5×) |

NPU INT8 is consistently 4.5–5.8× faster than CPU INT8 and 1.8–3.5× faster than GPU FP16 for these LLM-relevant matrix sizes. However, FP16 on NPU is catastrophically slow (up to 759× slower than CPU INT8), confirming that **quantization to INT8/INT4 is mandatory for NPU inference**.[^20]

***

## 4. Engineering Decision Matrix

| Decision | Recommended Path | Why |
|---|---|---|
| RL training cadence | Offline, scheduled (weekly/monthly) | Avoids per-rebuild energy cost, enables proper eval gates |
| Training infrastructure | Separate GPU service, not in Docker Compose stack | llama.cpp cannot do backprop; training needs PyTorch/TRL |
| Weight format | LoRA adapters merged into GGUF | Small delta, auditable, rollback = swap adapter file |
| Trajectory collection | Anonymized, append-only SQLite (extends Phase 5 audit) | Feeds offline training without runtime overhead |
| NPU integration | Hexagon backend for Android/Snapdragon targets; ANE disaggregated for Apple Silicon | Match hardware to workload phase (prefill vs decode) |
| NPU scheduling | Compile-time (current hardware limitation) | Runtime dynamic scheduling requires ISA extensions not yet shipping |
| Parallel inference | Batch filling (multiple requests per HMX tile) | Gets near-free throughput from otherwise wasted matrix unit capacity |

---

## References

1. [grpo-rl-training skill by orchestra-research/ai-research-skills](https://playbooks.com/skills/orchestra-research/ai-research-skills/grpo-rl-training) - This skill guides GRPO/RL fine-tuning with TRL to enforce formats, optimize reasoning, and align dom...

2. [FareedKhan-dev/multi-agent-training-grpo](https://github.com/FareedKhan-dev/multi-agent-training-grpo) - Group-Based Evaluation: GRPO evaluates multiple trajectories for the same query, allowing the agent ...

3. [Addressing Reward Hacking Explicitly - ApX Machine Learning](https://apxml.com/courses/rlhf-reinforcement-learning-human-feedback/chapter-6-advanced-rlhf-techniques/addressing-reward-hacking)

4. [Reinforcement Learning for LLMs: RLHF, DPO, and ...](https://www.inferless.com/learn/a-deep-dive-into-reinforcement-learning) - Prevent "catastrophic forgetting" of desirable behaviors. Mitigate the risk of the policy LLM findin...

5. [Steering RL Training: Benchmarking Interventions Against ...](https://www.alignmentforum.org/posts/R5MdWGKsuvdPwGFBG/steering-rl-training-benchmarking-interventions-against)

6. [[PDF] RL Methods for Mitigating Catastrophic Forgetting in Continual SLM ...](https://cs224r.stanford.edu/projects/pdfs/CS_224R_Project_Final_Report%20(1).pdf)

7. [Why On-Policy Reinforcement Learning Forgets Less](https://openreview.net/pdf/e39df6fed7d0b9869be14050ef3abd4f7934cf0f.pdf) - fine-tuning often leads to catastrophic forgetting—a phenomenon where learning new information. 87 s...

8. [Unleashing Efficient Asynchronous RL Post-Training via ...](https://www.arxiv.org/abs/2601.12784) - Reinforcement learning (RL) post-training has become pivotal for enhancing the capabilities of moder...

9. [Unleashing Efficient Asynchronous RL Post-Training via Staleness ...](https://arxiv.org/html/2601.12784v1)

10. [GRPO in Reinforcement Learning Explained - DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/group-relative-policy-optimization-reinforcement-learning) - Learn how Group Relative Policy Optimization improves reinforcement learning by aligning models with...

11. [Scaling LLM Test-Time Compute with Mobile NPU on ...](https://arxiv.org/html/2509.23324v1)

12. [Hardware-Assisted Virtualization of Neural Processing ...](https://platformxlab.github.io/papers/neu10-micro24.pdf) - by Y Xue · Cited by 8 — Our new architectural support enables Neu10 to offer the flexibility of NPU ...

13. [Scaling Intel Neural Processing Unit (NPU) in AI Client ...](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Scaling-Intel-Neural-Processing-Unit-NPU-in-AI-Client-Ecosystem/post/1574577) - Intel's NPU driver implements a D3D12 compliant user mode library to provide hardware acceleration f...

14. [eBPF Tutorial by Example: Tracing Intel NPU Kernel Driver ...](https://eunomia.dev/tutorials/xpu/npu-kernel-driver/) - Intel's NPU driver follows a two-layer architecture similar to GPU drivers. The kernel module ( inte...

15. [haozixu/llama.cpp-npu](https://github.com/haozixu/llama.cpp-npu) - This is the code repository for the paper Scaling LLM Test-Time Compute with Mobile NPU on Smartphon...

16. [Disaggregated Inference on Apple Silicon: NPU prefill and GPU ...](https://blog.squeezebits.com/disaggregated-inference-on-apple-silicon-npu-prefill-and-gpu-decode-67176) - In this article, we introduce how to run LLMs efficiently on Apple Silicon with disaggregated infere...

17. [Deploying Transformers on the Apple Neural Engine](https://machinelearning.apple.com/research/neural-engine-transformers) - It will help developers minimize the impact of their ML inference workloads on app memory, app respo...

18. [Neural Engine](https://dwhou.github.io/posts/2021-7-NPU_GPU_CPU/ANE.html)

19. [GitHub - zhouwg/ggml-hexagon: reference implementation of the backend for llama.cpp on Android phone equipped with Qualcomm's Hexagon NPU, details can be seen at https://github.com/zhouwg/ggml-hexagon/discussions/18](https://github.com/zhouwg/ggml-hexagon) - reference implementation of the backend for llama.cpp on Android phone equipped with Qualcomm's Hexa...

20. [Fast On-device LLM Inference with NPUs - arXiv.org](https://arxiv.org/html/2407.05858v2)

