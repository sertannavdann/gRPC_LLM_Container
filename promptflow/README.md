# Prompt Flow Integration

This directory contains Microsoft Prompt Flow configurations for visual workflow editing, evaluation, and prompt management.

## ğŸ“ Directory Structure

```
promptflow/
â”œâ”€â”€ connections/          # LLM provider connection configs
â”‚   â”œâ”€â”€ local_llm.yaml   # Local llama.cpp via gRPC
â”‚   â”œâ”€â”€ openai.yaml      # OpenAI GPT models
â”‚   â”œâ”€â”€ anthropic.yaml   # Anthropic Claude models
â”‚   â””â”€â”€ perplexity.yaml  # Perplexity Sonar models
â”œâ”€â”€ flows/
â”‚   â”œâ”€â”€ agent_workflow/  # Main agent DAG workflow
â”‚   â”‚   â”œâ”€â”€ flow.dag.yaml
â”‚   â”‚   â”œâ”€â”€ intent_analyzer.py
â”‚   â”‚   â”œâ”€â”€ context_retriever.py
â”‚   â”‚   â”œâ”€â”€ tool_selector.jinja2
â”‚   â”‚   â”œâ”€â”€ tool_executor.py
â”‚   â”‚   â””â”€â”€ synthesize_response.jinja2
â”‚   â””â”€â”€ evaluator/       # Evaluation workflow
â”‚       â”œâ”€â”€ flow.dag.yaml
â”‚       â”œâ”€â”€ run_agent.py
â”‚       â”œâ”€â”€ evaluate_tools.py
â”‚       â””â”€â”€ evaluate_answer.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ eval_cases.csv   # Test cases for batch evaluation
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ agent_system.yaml     # System prompts with variants
â”‚   â””â”€â”€ tool_selection.yaml   # Tool selection prompts
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Install Prompt Flow

```bash
pip install promptflow promptflow-tools promptflow-devkit
```

### 2. Set Up Connections (VS Code)

1. Open VS Code with Prompt Flow extension
2. Go to **Prompt Flow** sidebar (icon looks like a flow chart)
3. Click **Connections** â†’ **+** to add a connection
4. Select connection type (OpenAI, Anthropic, Custom) and configure

### 3. Set Up Connections (CLI)

```bash
# Set API keys as environment variables first
export OPENAI_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"

# Create connections
make pf-connection-openai
make pf-connection-anthropic
```

### 4. Run the Agent Workflow

```bash
# Test with a query
make pf-run Q="What is 25 * 17?"

# With debug output
make pf-run-debug Q="Calculate the square root of 144"
```

### 5. Run Batch Evaluation

```bash
make pf-eval
```

## ğŸ”§ Makefile Commands

| Command | Description |
|---------|-------------|
| `make pf-run Q="..."` | Run agent workflow with query |
| `make pf-run-debug Q="..."` | Run with debug output |
| `make pf-eval` | Run batch evaluation |
| `make pf-serve` | Serve flow as API (port 8080) |
| `make pf-trace` | Start trace UI (port 23333) |
| `make pf-connections` | List registered connections |
| `make pf-build` | Build Docker package |

## ğŸ“ Agent Workflow

The agent workflow is a 5-node DAG that mirrors the orchestrator logic:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   user_query     â”‚ (Input)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ intent_analyzer  â”‚ Pattern matching for tool detection
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚context_retriever â”‚ ChromaDB semantic search
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  tool_selector   â”‚ LLM-based tool selection (Jinja2)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  tool_executor   â”‚ gRPC tool execution
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚synthesize_responseâ”‚ Final response generation (Jinja2)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Evaluation Framework

### Test Cases (`data/eval_cases.csv`)

| Field | Description |
|-------|-------------|
| `query` | User input query |
| `expected_tools` | Comma-separated expected tool names |
| `expected_answer_contains` | Keywords that should appear in answer |
| `category` | Test category (math, code, search, conversation) |

### Evaluation Metrics

- **Tool Precision**: % of selected tools that were correct
- **Tool Recall**: % of expected tools that were selected
- **Tool F1**: Harmonic mean of precision and recall
- **Answer Match**: Whether expected keywords appear in answer

## ğŸ”„ A/B Testing Prompts

### System Prompt Variants (`prompts/agent_system.yaml`)

| Variant | Description |
|---------|-------------|
| `concise` | Brief, focused responses |
| `detailed` | Comprehensive explanations |
| `professional` | Formal, business-oriented tone |

### Usage in Flow

```yaml
# In flow.dag.yaml, reference variants:
inputs:
  prompt_variant:
    type: string
    default: "concise"
```

## ğŸ”Œ Connection Types

### Local LLM (gRPC)

```yaml
name: local_llm
type: custom
module: llm_service.grpc_client
configs:
  host: localhost
  port: 50051
```

### OpenAI

```yaml
name: openai_connection
type: open_ai
api_key: ${env:OPENAI_API_KEY}
api_base: https://api.openai.com/v1
```

### Anthropic

```yaml
name: anthropic_connection
type: custom
configs:
  api_key: ${env:ANTHROPIC_API_KEY}
  model: claude-sonnet-4-20250514
```

## ğŸ³ Serving as Docker

```bash
# Build Docker image
make pf-build

# Run the container
docker run -p 8080:8080 agent-workflow:latest
```

## ğŸ“¡ API Endpoints (when served)

When running `make pf-serve`:

```bash
# Health check
curl http://localhost:8080/health

# Run flow
curl -X POST http://localhost:8080/score \
  -H "Content-Type: application/json" \
  -d '{"user_query": "What is 5 + 3?", "debug_mode": false}'
```

## ğŸ” Tracing & Debugging

### Start Trace UI

```bash
make pf-trace
# Open http://localhost:23333
```

### View Traces in VS Code

1. Open Prompt Flow sidebar
2. Click **Trace Collections**
3. Select a trace to view execution details

## ğŸ¯ Best Practices

1. **Version prompts**: Use `prompts/` directory for all prompt templates
2. **Test changes**: Run `make pf-eval` before deploying changes
3. **Use variants**: A/B test prompts with the variants system
4. **Monitor traces**: Check trace UI for performance issues
5. **Batch test**: Use `data/eval_cases.csv` for regression testing
