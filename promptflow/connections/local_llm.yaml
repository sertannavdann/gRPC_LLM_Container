# Local LLM Connection (llama.cpp via gRPC)
# This connection points to the local llm_service container
$schema: https://azuremlschemas.azureedge.net/promptflow/latest/CustomConnection.schema.json
name: local_llm
type: custom
configs:
  endpoint: "localhost:50051"
  model: "qwen2.5-3b-instruct-q5_k_m"
  provider: "local"
  timeout: "60"
secrets:
  # No API key needed for local model
  api_key: "not-required"
