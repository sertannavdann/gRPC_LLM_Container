syntax = "proto3";
package llm;

service LLMService {
  // Streaming text generation
  rpc Generate(GenerateRequest) returns (stream GenerateResponse);

  // Batch generation for self-consistency scoring (Phase 2: Agent0)
  rpc GenerateBatch(GenerateBatchRequest) returns (GenerateBatchResponse);

  // LIDM: Model introspection RPCs
  rpc GetActiveModel(GetActiveModelRequest) returns (GetActiveModelResponse);
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse);
}

message GenerateRequest {
  string prompt = 1;
  int32 max_tokens = 2;
  float temperature = 3;
  string response_format = 4;
}

message GenerateResponse {
  string token = 1;
  bool is_final = 2;
  bool is_valid_json = 3;
}

// Phase 2: Batch generation for self-consistency scoring
message GenerateBatchRequest {
  string prompt = 1;
  int32 num_samples = 2;      // k responses to generate
  int32 max_tokens = 3;
  float temperature = 4;
  string response_format = 5;
}

message GenerateBatchResponse {
  repeated string responses = 1;           // k generated responses
  float self_consistency_score = 2;        // pÌ‚ = proportion agreeing with majority
  string majority_answer = 3;              // Most common answer
  int32 majority_count = 4;                // How many responses agree
}

// LIDM: Model introspection messages
message GetActiveModelRequest {}
message GetActiveModelResponse {
  string model_name = 1;
  string model_filename = 2;
  int32 context_window = 3;
  int32 max_tokens = 4;
  repeated string capabilities = 5;
  string tier = 6;
  string backend = 7;
}

message ListModelsRequest {}
message ModelInfo {
  string filename = 1;
  string name = 2;
  int32 context_window = 3;
  int32 recommended_ctx = 4;
  repeated string capabilities = 5;
  string tier = 6;
}
message ListModelsResponse {
  repeated ModelInfo models = 1;
  string active_model = 2;
}
