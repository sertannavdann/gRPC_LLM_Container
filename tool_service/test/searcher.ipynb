{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver import ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from cachetools import cached, TTLCache\n",
    "\n",
    "class ScrapingService:\n",
    "    def __init__(self):\n",
    "        self.proxies = []\n",
    "        self.cache = TTLCache(maxsize=1000, ttl=3600)\n",
    "        self.CREDIBLE_DOMAINS = {\".gov\", \".edu\", \".ac.\", \"wikipedia.org\", \"who.int\"}\n",
    "        self.TRUSTED_TLDS = {\".org\", \".gov\", \".edu\"}\n",
    "        self.USER_AGENTS = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "        ]\n",
    "        \n",
    "        self.SEARCH_API_KEY = \"AIzaSyAThHluWfPfcDLKELraJS2RTGJhih3BOEM\"\n",
    "        self.SEARCH_ENGINE_ID = \"41b7b35fba4b24f40\"\n",
    "        self.SEARCH_API_URL = \"https://www.googleapis.com/customsearch/v1\"\n",
    "\n",
    "service = ScrapingService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_selenium_scrape(url: str, return_html: bool = False):\n",
    "    \"\"\"Test the selenium-based scraping\"\"\"\n",
    "    try:\n",
    "        options = ChromeOptions()\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(f\"user-agent={random.choice(service.USER_AGENTS)}\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "        # Fix for M1/M2 Macs\n",
    "        options.add_argument(\"--remote-debugging-port=9222\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        # Specify the correct ChromeDriver path\n",
    "        try:\n",
    "            chrome_service = Service(ChromeDriverManager().install())\n",
    "            driver = webdriver.Chrome(service=chrome_service, options=options)\n",
    "        except Exception as e:\n",
    "            print(f\"ChromeDriver error: {str(e)}\")\n",
    "            print(\"Trying with direct ChromeDriver path...\")\n",
    "            # Fallback to system Chrome if installed\n",
    "            options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nFetching {url} with Selenium...\")\n",
    "            driver.get(url)\n",
    "            html = driver.page_source\n",
    "            \n",
    "            if return_html:\n",
    "                return html\n",
    "                \n",
    "            # Test sanitization\n",
    "            try:\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "            except:\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                \n",
    "            for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"iframe\", \"noscript\"]):\n",
    "                element.decompose()\n",
    "            sanitized = soup.get_text(separator='\\n', strip=True)[:15000]\n",
    "            \n",
    "            print(\"\\nSuccessfully scraped with Selenium\")\n",
    "            print(f\"Content length: {len(sanitized)} characters\")\n",
    "            print(\"\\nSample content:\")\n",
    "            print(sanitized[:500] + \"...\")\n",
    "            \n",
    "            return sanitized\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"Selenium scrape failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search(query: str, max_results: int = 3):\n",
    "    \"\"\"Test the search engine functionality\"\"\"\n",
    "    if not service.SEARCH_API_KEY or service.SEARCH_API_KEY == \"YOUR_API_KEY\":\n",
    "        print(\"Search test skipped - No API key configured\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        params = {\n",
    "            \"key\": service.SEARCH_API_KEY,\n",
    "            \"cx\": service.SEARCH_ENGINE_ID,\n",
    "            \"q\": query,\n",
    "            \"num\": max_results,\n",
    "            \"safe\": \"active\"\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTesting search for: '{query}'\")\n",
    "        resp = requests.get(service.SEARCH_API_URL, params=params, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        results = resp.json().get('items', [])\n",
    "        \n",
    "        print(f\"Found {len(results)} results:\")\n",
    "        for i, item in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {item.get('title')}\")\n",
    "            print(f\"URL: {item.get('link')}\")\n",
    "            print(f\"Snippet: {item.get('snippet')}\")\n",
    "            \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Search failed: {str(e)}\")\n",
    "        if hasattr(e, 'response') and e.response.status_code == 403:\n",
    "            print(\"API key may be invalid or quota exceeded\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_basic_scrape(url: str):\n",
    "    \"\"\"Test the basic web scraping functionality\"\"\"\n",
    "    try:\n",
    "        # First try with requests\n",
    "        headers = {\"User-Agent\": random.choice(service.USER_AGENTS)}\n",
    "        print(f\"Testing basic scrape of {url} with requests...\")\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            html = resp.text\n",
    "            print(\"Success with direct requests\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Requests failed, trying with selenium: {str(e)}\")\n",
    "            html = test_selenium_scrape(url, return_html=True)\n",
    "            if not html:\n",
    "                raise ValueError(\"Both requests and selenium failed\")\n",
    "        \n",
    "        # Test sanitize_content - using 'html.parser' as fallback\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "        except:\n",
    "            print(\"Falling back to html.parser\")\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"iframe\", \"noscript\"]):\n",
    "            element.decompose()\n",
    "        sanitized = soup.get_text(separator='\\n', strip=True)[:15000]\n",
    "        \n",
    "        print(f\"\\nScraped content length: {len(sanitized)} characters\")\n",
    "        print(\"\\nSample content:\")\n",
    "        print(sanitized[:500] + \"...\")\n",
    "        \n",
    "        return sanitized\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_credible_source(url: str):\n",
    "    \"\"\"Test the credible source detection\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    is_credible = any(tld in parsed.netloc for tld in service.TRUSTED_TLDS) or \\\n",
    "                 any(domain in parsed.netloc for domain in service.CREDIBLE_DOMAINS)\n",
    "    \n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"Domain: {parsed.netloc}\")\n",
    "    print(f\"Is credible source: {is_credible}\")\n",
    "    return is_credible\n",
    "\n",
    "# Example usage:\n",
    "# test_credible_source(\"https://www.nih.gov\")\n",
    "# test_credible_source(\"https://www.wikipedia.org\")\n",
    "# test_credible_source(\"https://www.example.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Basic Scraping ===\n",
      "Testing basic scrape of https://httpbin.org/html with requests...\n",
      "Success with direct requests\n",
      "\n",
      "Scraped content length: 3594 characters\n",
      "\n",
      "Sample content:\n",
      "Herman Melville - Moby-Dick\n",
      "Availing himself of the mild, summer-cool weather that now reigned in these latitudes, and in preparation for the peculiarly active pursuits shortly to be anticipated, Perth, the begrimed, blistered old blacksmith, had not removed his portable forge to the hold again, after concluding his contributory work for Ahab's leg, but still retained it on deck, fast lashed to ringbolts by the foremast; being now almost incessantly invoked by the headsmen, and harpooneers, and ...\n",
      "\n",
      "=== Testing Search ===\n",
      "\n",
      "Testing search for: 'python programming'\n",
      "Found 3 results:\n",
      "\n",
      "1. Welcome to Python.org\n",
      "URL: https://www.python.org/\n",
      "Snippet: The official home of the Python Programming Language.\n",
      "\n",
      "2. Introduction to Python\n",
      "URL: https://www.w3schools.com/python/python_intro.asp\n",
      "Snippet: Python Syntax compared to other programming languages · Python was designed for readability, and has some similarities to the English language with influence ...\n",
      "\n",
      "3. Python For Beginners | Python.org\n",
      "URL: https://www.python.org/about/gettingstarted/\n",
      "Snippet: An experienced programmer in any programming language (whatever it may be) can pick up Python very quickly. It's also easy for beginners to use and learn.\n",
      "\n",
      "=== Testing Selenium ===\n",
      "ChromeDriver error: [Errno 8] Exec format error: '/Users/sertanavdan/.wdm/drivers/chromedriver/mac64/135.0.7049.42/chromedriver-mac-arm64/THIRD_PARTY_NOTICES.chromedriver'\n",
      "Trying with direct ChromeDriver path...\n",
      "\n",
      "Fetching https://httpbin.org/html with Selenium...\n",
      "\n",
      "Successfully scraped with Selenium\n",
      "Content length: 3594 characters\n",
      "\n",
      "Sample content:\n",
      "Herman Melville - Moby-Dick\n",
      "Availing himself of the mild, summer-cool weather that now reigned in these latitudes, and in preparation for the peculiarly active pursuits shortly to be anticipated, Perth, the begrimed, blistered old blacksmith, had not removed his portable forge to the hold again, after concluding his contributory work for Ahab's leg, but still retained it on deck, fast lashed to ringbolts by the foremast; being now almost incessantly invoked by the headsmen, and harpooneers, and ...\n",
      "\n",
      "=== Testing Credible Sources ===\n",
      "URL: https://www.cdc.gov\n",
      "Domain: www.cdc.gov\n",
      "Is credible source: True\n",
      "URL: https://www.nytimes.com\n",
      "Domain: www.nytimes.com\n",
      "Is credible source: False\n",
      "URL: https://www.harvard.edu\n",
      "Domain: www.harvard.edu\n",
      "Is credible source: True\n",
      "\n",
      "=== Test Summary ===\n",
      "basic_scrape: PASSED\n",
      "search: PASSED\n",
      "selenium: PASSED\n"
     ]
    }
   ],
   "source": [
    "def run_all_tests():\n",
    "    \"\"\"Run all test cases with error handling\"\"\"\n",
    "    print(\"=== Testing Basic Scraping ===\")\n",
    "    basic_result = test_basic_scrape(\"https://httpbin.org/html\")  # Using a reliable test URL\n",
    "    \n",
    "    print(\"\\n=== Testing Search ===\")\n",
    "    search_results = test_search(\"python programming\", max_results=3) if service.SEARCH_API_KEY != \"YOUR_API_KEY\" else None\n",
    "    \n",
    "    print(\"\\n=== Testing Selenium ===\")\n",
    "    selenium_result = test_selenium_scrape(\"https://httpbin.org/html\")\n",
    "    \n",
    "    print(\"\\n=== Testing Credible Sources ===\")\n",
    "    test_credible_source(\"https://www.cdc.gov\")\n",
    "    test_credible_source(\"https://www.nytimes.com\")\n",
    "    test_credible_source(\"https://www.harvard.edu\")\n",
    "    \n",
    "    return {\n",
    "        \"basic_scrape\": bool(basic_result),\n",
    "        \"search\": bool(search_results) if search_results is not None else \"skipped\",\n",
    "        \"selenium\": bool(selenium_result)\n",
    "    }\n",
    "\n",
    "# Run the tests\n",
    "test_results = run_all_tests()\n",
    "print(\"\\n=== Test Summary ===\")\n",
    "for test, result in test_results.items():\n",
    "    status = \"PASSED\" if result else \"FAILED\" if result is not True else \"SKIPPED\"\n",
    "    print(f\"{test}: {status}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
